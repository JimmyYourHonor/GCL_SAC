{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jif055/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import pybullet_envs\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from cost import CostNN\n",
    "from sac_torch import Agent\n",
    "\n",
    "\n",
    "# ENV SETUP\n",
    "env_name = 'InvertedPendulumBulletEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    G = np.zeros_like(rewards, dtype=float)\n",
    "    G[-1] = rewards[-1]\n",
    "    for idx in range(-2, -len(rewards)-1, -1):\n",
    "        G[idx] = rewards[idx] + gamma * G[idx+1]\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITILIZING POLICY AND REWARD FUNCTION\n",
    "policy = Agent(input_dims=env.observation_space.shape[0], env=env, \n",
    "                n_actions=env.action_space.shape[0])\n",
    "cost_f = CostNN(env.observation_space.shape[0] + env.action_space.shape[0])\n",
    "policy_optimizer = torch.optim.Adam(policy.actor.parameters(), 3e-4)\n",
    "cost_optimizer = torch.optim.Adam(cost_f.parameters(), 1e-2, weight_decay=1e-4)\n",
    "\n",
    "mean_rewards = []\n",
    "mean_costs = []\n",
    "mean_loss_rew = []\n",
    "EPISODES_TO_PLAY = 1\n",
    "REWARD_FUNCTION_UPDATE = 10\n",
    "DEMO_BATCH = 100\n",
    "\n",
    "D_demo_states = np.load('expert_samples/sac_inverted_pendulum_states.npy',allow_pickle=True)\n",
    "D_demo_actions = np.load('expert_samples/sac_inverted_pendulum_actions.npy',allow_pickle=True)\n",
    "D_demo_probs = np.load('expert_samples/sac_inverted_pendulum_probs.npy',allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:23.0 loss: 14.912930488586426\n",
      "mean reward:25.272727272727273 loss: 14.75391960144043\n",
      "mean reward:19.476190476190474 loss: 14.76296329498291\n",
      "mean reward:16.580645161290324 loss: 14.781837463378906\n",
      "mean reward:15.048780487804878 loss: 14.753790855407715\n",
      "mean reward:13.862745098039216 loss: 14.763008117675781\n",
      "mean reward:12.934426229508198 loss: 14.719185829162598\n",
      "mean reward:12.394366197183098 loss: 14.733758926391602\n",
      "mean reward:12.012345679012345 loss: 14.745471954345703\n",
      "mean reward:11.67032967032967 loss: 14.721084594726562\n",
      "mean reward:11.445544554455445 loss: 14.700095176696777\n",
      "mean reward:11.225225225225225 loss: 14.733851432800293\n",
      "mean reward:11.008264462809917 loss: 14.737875938415527\n",
      "mean reward:10.854961832061068 loss: 14.733845710754395\n",
      "mean reward:10.72340425531915 loss: 14.715564727783203\n",
      "mean reward:10.635761589403973 loss: 14.720388412475586\n",
      "mean reward:10.5527950310559 loss: 14.712876319885254\n",
      "mean reward:10.526315789473685 loss: 14.718283653259277\n",
      "mean reward:10.458563535911603 loss: 14.714822769165039\n",
      "mean reward:10.38219895287958 loss: 14.741926193237305\n",
      "mean reward:10.283582089552239 loss: 14.771583557128906\n",
      "mean reward:10.24170616113744 loss: 14.73532772064209\n",
      "mean reward:10.20814479638009 loss: 14.752157211303711\n",
      "mean reward:10.134199134199134 loss: 14.729312896728516\n",
      "mean reward:10.08298755186722 loss: 14.74974536895752\n",
      "mean reward:10.03187250996016 loss: 14.748260498046875\n",
      "mean reward:10.015325670498084 loss: 14.727611541748047\n",
      "mean reward:9.966789667896679 loss: 14.703960418701172\n",
      "mean reward:9.95017793594306 loss: 14.723763465881348\n",
      "mean reward:9.927835051546392 loss: 14.728348731994629\n",
      "mean reward:9.916943521594684 loss: 14.742798805236816\n",
      "mean reward:9.877813504823152 loss: 14.72516918182373\n",
      "mean reward:9.853582554517134 loss: 14.713069915771484\n",
      "mean reward:9.80060422960725 loss: 14.7266263961792\n",
      "mean reward:9.762463343108504 loss: 14.74813175201416\n",
      "mean reward:9.735042735042734 loss: 14.721573829650879\n",
      "mean reward:9.725761772853186 loss: 14.7439546585083\n",
      "mean reward:9.700808625336927 loss: 14.724329948425293\n",
      "mean reward:9.68766404199475 loss: 14.717278480529785\n",
      "mean reward:9.690537084398978 loss: 14.764368057250977\n",
      "mean reward:9.670822942643392 loss: 14.714069366455078\n",
      "mean reward:9.652068126520682 loss: 14.709674835205078\n",
      "mean reward:9.6270783847981 loss: 14.734649658203125\n",
      "mean reward:9.60324825986079 loss: 14.71609878540039\n",
      "mean reward:9.598639455782314 loss: 14.723126411437988\n",
      "mean reward:9.60088691796009 loss: 14.709563255310059\n",
      "mean reward:9.579175704989154 loss: 14.729819297790527\n",
      "mean reward:9.562632696390658 loss: 14.736602783203125\n",
      "mean reward:9.555093555093555 loss: 14.73056697845459\n",
      "mean reward:9.54786150712831 loss: 14.732301712036133\n",
      "mean reward:9.546906187624751 loss: 14.730851173400879\n",
      "mean reward:9.549902152641879 loss: 14.709493637084961\n",
      "mean reward:9.5489443378119 loss: 14.692749977111816\n",
      "mean reward:9.54990583804143 loss: 14.729024887084961\n",
      "mean reward:9.548983364140481 loss: 14.704745292663574\n",
      "mean reward:9.539019963702358 loss: 14.707015037536621\n",
      "mean reward:9.520499108734402 loss: 14.724498748779297\n",
      "mean reward:9.499124343257444 loss: 14.730412483215332\n",
      "mean reward:9.502581755593804 loss: 14.715932846069336\n",
      "mean reward:9.505922165820643 loss: 14.707958221435547\n",
      "mean reward:9.502495840266223 loss: 14.713968276977539\n",
      "mean reward:9.494271685761047 loss: 14.708267211914062\n",
      "mean reward:9.494363929146537 loss: 14.717857360839844\n",
      "mean reward:9.492868462757528 loss: 14.716001510620117\n",
      "mean reward:9.497659906396256 loss: 14.734420776367188\n",
      "mean reward:9.497695852534562 loss: 14.717875480651855\n",
      "mean reward:9.48108925869894 loss: 14.705077171325684\n",
      "mean reward:9.482861400894187 loss: 14.717124938964844\n",
      "mean reward:9.490455212922173 loss: 14.692551612854004\n",
      "mean reward:9.476121562952244 loss: 14.699968338012695\n",
      "mean reward:9.477888730385164 loss: 14.696015357971191\n",
      "mean reward:9.471167369901547 loss: 14.695119857788086\n",
      "mean reward:9.48127600554785 loss: 14.722187042236328\n",
      "mean reward:9.477428180574556 loss: 14.740062713623047\n",
      "mean reward:9.479082321187585 loss: 14.70097541809082\n",
      "mean reward:9.468708388814914 loss: 14.724864959716797\n",
      "mean reward:9.467805519053876 loss: 14.728989601135254\n",
      "mean reward:9.469520103761349 loss: 14.729816436767578\n",
      "mean reward:9.46222791293214 loss: 14.737220764160156\n",
      "mean reward:9.448798988621997 loss: 14.725132942199707\n",
      "mean reward:9.446941323345818 loss: 14.707376480102539\n",
      "mean reward:9.438964241676942 loss: 14.747759819030762\n",
      "mean reward:9.433617539585871 loss: 14.719009399414062\n",
      "mean reward:9.425992779783394 loss: 14.69168758392334\n",
      "mean reward:9.423305588585018 loss: 14.710104942321777\n",
      "mean reward:9.417156286721504 loss: 14.720768928527832\n",
      "mean reward:9.422764227642276 loss: 14.679773330688477\n",
      "mean reward:9.415614236509759 loss: 14.712865829467773\n",
      "mean reward:9.414301929625426 loss: 14.728221893310547\n",
      "mean reward:9.414141414141413 loss: 14.70999526977539\n",
      "mean reward:9.413984461709212 loss: 14.73193359375\n",
      "mean reward:9.403951701427003 loss: 14.727931022644043\n",
      "mean reward:9.396308360477741 loss: 14.717042922973633\n",
      "mean reward:9.39312567132116 loss: 14.7086181640625\n",
      "mean reward:9.396386822529225 loss: 14.730764389038086\n",
      "mean reward:9.391167192429021 loss: 14.685811042785645\n",
      "mean reward:9.392299687825183 loss: 14.693702697753906\n",
      "mean reward:9.388259526261585 loss: 14.723435401916504\n",
      "mean reward:9.37308868501529 loss: 14.702256202697754\n",
      "mean reward:9.373360242179617 loss: 14.731956481933594\n"
     ]
    }
   ],
   "source": [
    "cost_f = cost_f.cuda()\n",
    "return_list, sum_of_cost_list = [], []\n",
    "D_sample_states = np.array([])\n",
    "D_sample_actions = np.array([])\n",
    "D_sample_probs = np.array([])\n",
    "for i in range(1000):\n",
    "    #trajs = [policy.generate_session(env) for _ in range(EPISODES_TO_PLAY)]\n",
    "    #sample_trajs = trajs + sample_trajs\n",
    "    #D_samp = preprocess_traj(trajs, D_samp)\n",
    "    state, prob, action, rewards = policy.generate_session(env)\n",
    "    D_sample_states = np.concatenate((D_sample_states, state.reshape(-1)))\n",
    "    D_sample_actions = np.concatenate((D_sample_actions, action))\n",
    "    D_sample_probs = np.concatenate((D_sample_probs, prob))\n",
    "    # UPDATING REWARD FUNCTION (TAKES IN D_samp, D_demo)\n",
    "    loss_rew = []\n",
    "    for _ in range(REWARD_FUNCTION_UPDATE):\n",
    "        D_samp = D_sample_states.reshape(-1,5)\n",
    "        selected_samp = np.random.choice(len(D_samp), DEMO_BATCH)\n",
    "        selected_demo = np.random.choice(len(D_demo_states), DEMO_BATCH)\n",
    "\n",
    "        D_s_samp_states  = D_samp[selected_samp]\n",
    "        D_s_samp_actions = D_sample_actions[selected_samp]\n",
    "        D_s_samp_probs   = D_sample_probs[selected_samp]\n",
    "        D_s_demo_states  = D_demo_states[selected_demo]\n",
    "        D_s_demo_actions = D_demo_actions[selected_demo]\n",
    "        D_s_demo_probs   = D_demo_probs[selected_demo]\n",
    "\n",
    "        #D̂ samp ← D̂ demo ∪ D̂ samp\n",
    "        states  = np.concatenate((D_s_demo_states, D_s_samp_states), axis = 0)\n",
    "        actions = np.concatenate((D_s_demo_actions, D_s_samp_actions), axis = 0)\n",
    "        probs   = np.concatenate((D_s_demo_probs, D_s_samp_probs), axis = 0)\n",
    "\n",
    "#         states, probs, actions = D_s_samp[:,:-2], D_s_samp[:,-2], D_s_samp[:,-1]\n",
    "#         states_expert, actions_expert = D_s_demo[:,:-2], D_s_demo[:,-1]\n",
    "        states_expert = D_s_demo_states\n",
    "        actions_expert = D_s_demo_actions\n",
    "\n",
    "        # Reducing from float64 to float32 for making computaton faster\n",
    "        states = torch.tensor(states, dtype=torch.float32).cuda()\n",
    "        probs = torch.tensor(probs, dtype=torch.float32).cuda()\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).cuda()\n",
    "        states_expert = torch.tensor(states_expert, dtype=torch.float32).cuda()\n",
    "        actions_expert = torch.tensor(actions_expert, dtype=torch.float32).cuda()\n",
    "\n",
    "        costs_samp = cost_f(torch.cat((states, actions.reshape(-1, 1)), dim=-1))\n",
    "        costs_demo = cost_f(torch.cat((states_expert, actions_expert.reshape(-1, 1)), dim=-1))\n",
    "\n",
    "        probs = torch.exp(probs)\n",
    "        # LOSS CALCULATION FOR IOC (COST FUNCTION)\n",
    "        loss_IOC = torch.mean(costs_demo) + \\\n",
    "                torch.log(torch.mean(torch.exp(-costs_samp)/(probs+1e-7)))\n",
    "        # UPDATING THE COST FUNCTION\n",
    "        cost_optimizer.zero_grad()\n",
    "        loss_IOC.backward()\n",
    "        cost_optimizer.step()\n",
    "\n",
    "        loss_rew.append(loss_IOC.detach().cpu().numpy())\n",
    "\n",
    "    for _ in range(EPISODES_TO_PLAY):\n",
    "        \n",
    "        states = torch.tensor(state, dtype=torch.float32).cuda()\n",
    "        actions = torch.tensor(action, dtype=torch.float32).cuda()\n",
    "            \n",
    "        costs = cost_f(torch.cat((states, actions.reshape(-1, 1)), dim=-1)).detach().cpu().numpy()\n",
    "        cumulative_returns = np.array(get_cumulative_rewards(-costs, 0.99))\n",
    "        cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32).cuda()\n",
    "\n",
    "        mu, sigma = policy.actor(states)\n",
    "        distribution = Normal(mu, sigma)\n",
    "#         logits = policy(states)\n",
    "#         probs = nn.functional.softmax(logits, -1)\n",
    "#         log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "#         log_probs_for_actions = torch.sum(\n",
    "#             log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "        log_probs -= torch.log(1-actions.pow(2)+policy.actor.reparam_noise)\n",
    "        log_probs = log_probs.sum(1, keepdim=True)\n",
    "        probs = torch.exp(log_probs)\n",
    "    \n",
    "        entropy = -torch.mean(torch.sum(probs*log_probs), dim = -1 )\n",
    "        loss = -torch.mean(log_probs*cumulative_returns -entropy*1e-2) \n",
    "\n",
    "        # UPDATING THE POLICY NETWORK\n",
    "        policy_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "    returns = np.sum(rewards)\n",
    "    sum_of_cost = np.sum(costs)\n",
    "    return_list.append(returns)\n",
    "    sum_of_cost_list.append(sum_of_cost)\n",
    "\n",
    "    mean_rewards.append(np.mean(return_list))\n",
    "    mean_costs.append(np.mean(sum_of_cost_list))\n",
    "    mean_loss_rew.append(np.mean(loss_rew))\n",
    "\n",
    "    # PLOTTING PERFORMANCE\n",
    "    if i % 10 == 0:\n",
    "        # clear_output(True)\n",
    "        print(f\"mean reward:{np.mean(return_list)} loss: {loss_IOC}\")\n",
    "\n",
    "        plt.figure(figsize=[16, 12])\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(f\"Mean reward per {EPISODES_TO_PLAY} games\")\n",
    "        plt.plot(mean_rewards)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(f\"Mean cost per {EPISODES_TO_PLAY} games\")\n",
    "        plt.plot(mean_costs)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(f\"Mean loss per {REWARD_FUNCTION_UPDATE} batches\")\n",
    "        plt.plot(mean_loss_rew)\n",
    "        plt.grid()\n",
    "\n",
    "        # plt.show()\n",
    "        plt.savefig('plots/GCL_learning_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    if np.mean(return_list) > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-latest",
   "language": "python",
   "name": "ml-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
