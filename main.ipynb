{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jif055/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import pybullet_envs\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from cost import CostNN\n",
    "from sac_torch import Agent\n",
    "\n",
    "\n",
    "# ENV SETUP\n",
    "env_name = 'InvertedPendulumBulletEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    G = np.zeros_like(rewards, dtype=float)\n",
    "    G[-1] = rewards[-1]\n",
    "    for idx in range(-2, -len(rewards)-1, -1):\n",
    "        G[idx] = rewards[idx] + gamma * G[idx+1]\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITILIZING POLICY AND REWARD FUNCTION\n",
    "policy = Agent(input_dims=env.observation_space.shape[0], env=env, \n",
    "                n_actions=env.action_space.shape[0])\n",
    "cost_f = CostNN(env.observation_space.shape[0] + env.action_space.shape[0])\n",
    "policy_optimizer = torch.optim.Adam(policy.actor.parameters(), 3e-4)\n",
    "cost_optimizer = torch.optim.Adam(cost_f.parameters(), 1e-2, weight_decay=1e-4)\n",
    "\n",
    "mean_rewards = []\n",
    "mean_costs = []\n",
    "mean_loss_rew = []\n",
    "mean_policy_loss = []\n",
    "EPISODES_TO_PLAY = 1\n",
    "REWARD_FUNCTION_UPDATE = 10\n",
    "DEMO_BATCH = 100\n",
    "\n",
    "D_demo_states = np.load('expert_samples/sac_inverted_pendulum_states.npy',allow_pickle=True)\n",
    "D_demo_actions = np.load('expert_samples/sac_inverted_pendulum_actions.npy',allow_pickle=True)\n",
    "D_demo_probs = np.load('expert_samples/sac_inverted_pendulum_probs.npy',allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:27.0 loss: 15.1333646774292\n",
      "mean reward:22.454545454545453 loss: 14.933072090148926\n",
      "mean reward:26.571428571428573 loss: 14.810850143432617\n",
      "mean reward:25.903225806451612 loss: 14.832557678222656\n",
      "mean reward:25.365853658536587 loss: 14.829389572143555\n",
      "mean reward:24.372549019607842 loss: 14.81758975982666\n",
      "mean reward:23.934426229508198 loss: 14.7814302444458\n",
      "mean reward:24.464788732394368 loss: 14.804716110229492\n",
      "mean reward:25.11111111111111 loss: 14.860937118530273\n",
      "mean reward:24.87912087912088 loss: 14.84914779663086\n",
      "mean reward:25.633663366336634 loss: 14.832154273986816\n",
      "mean reward:25.00900900900901 loss: 14.782355308532715\n",
      "mean reward:25.05785123966942 loss: 14.831594467163086\n",
      "mean reward:25.633587786259543 loss: 14.78981876373291\n",
      "mean reward:25.638297872340427 loss: 14.841951370239258\n",
      "mean reward:25.920529801324502 loss: 14.787618637084961\n",
      "mean reward:25.48447204968944 loss: 14.811004638671875\n",
      "mean reward:25.935672514619885 loss: 14.796098709106445\n",
      "mean reward:25.734806629834253 loss: 14.811727523803711\n",
      "mean reward:25.73821989528796 loss: 14.772706985473633\n",
      "mean reward:25.90049751243781 loss: 14.790563583374023\n",
      "mean reward:26.24170616113744 loss: 14.810883522033691\n",
      "mean reward:26.266968325791854 loss: 14.751548767089844\n",
      "mean reward:26.294372294372295 loss: 14.842982292175293\n",
      "mean reward:26.219917012448132 loss: 14.80064868927002\n",
      "mean reward:26.47808764940239 loss: 14.77664852142334\n",
      "mean reward:26.371647509578544 loss: 14.811948776245117\n",
      "mean reward:26.33210332103321 loss: 14.767107963562012\n",
      "mean reward:26.391459074733095 loss: 14.799663543701172\n",
      "mean reward:26.415807560137456 loss: 14.81218433380127\n",
      "mean reward:26.365448504983387 loss: 14.746850967407227\n",
      "mean reward:26.318327974276528 loss: 14.801973342895508\n",
      "mean reward:26.34890965732087 loss: 14.767394065856934\n",
      "mean reward:26.637462235649547 loss: 14.795879364013672\n",
      "mean reward:26.712609970674485 loss: 14.742321968078613\n",
      "mean reward:26.897435897435898 loss: 14.784960746765137\n",
      "mean reward:26.806094182825486 loss: 14.76572322845459\n",
      "mean reward:26.91374663072776 loss: 14.765654563903809\n",
      "mean reward:26.926509186351705 loss: 14.754888534545898\n",
      "mean reward:26.938618925831204 loss: 14.795778274536133\n",
      "mean reward:26.887780548628427 loss: 14.752103805541992\n",
      "mean reward:26.97566909975669 loss: 14.757364273071289\n",
      "mean reward:26.897862232779097 loss: 14.771491050720215\n",
      "mean reward:27.00232018561485 loss: 14.775105476379395\n",
      "mean reward:27.31519274376417 loss: 14.750763893127441\n",
      "mean reward:27.33259423503326 loss: 14.74663257598877\n",
      "mean reward:27.373101952277658 loss: 14.77005386352539\n",
      "mean reward:27.360934182590235 loss: 14.757304191589355\n",
      "mean reward:27.413721413721415 loss: 14.766656875610352\n",
      "mean reward:27.380855397148675 loss: 14.742218971252441\n",
      "mean reward:27.477045908183634 loss: 14.844762802124023\n",
      "mean reward:27.610567514677104 loss: 14.730926513671875\n",
      "mean reward:27.671785028790786 loss: 14.77670669555664\n",
      "mean reward:27.661016949152543 loss: 14.776819229125977\n",
      "mean reward:27.648798521256932 loss: 14.732234001159668\n",
      "mean reward:27.687840290381125 loss: 14.777785301208496\n",
      "mean reward:27.568627450980394 loss: 14.747056007385254\n",
      "mean reward:27.51488616462347 loss: 14.73664379119873\n",
      "mean reward:27.6368330464716 loss: 14.763023376464844\n",
      "mean reward:27.593908629441625 loss: 14.740641593933105\n",
      "mean reward:27.640599001663894 loss: 14.768156051635742\n",
      "mean reward:27.693944353518823 loss: 14.754816055297852\n",
      "mean reward:27.60708534621578 loss: 14.772004127502441\n",
      "mean reward:27.58637083993661 loss: 14.725751876831055\n",
      "mean reward:27.5195007800312 loss: 14.761801719665527\n",
      "mean reward:27.580645161290324 loss: 14.770383834838867\n",
      "mean reward:27.571860816944024 loss: 14.757054328918457\n",
      "mean reward:27.68703427719821 loss: 14.765049934387207\n",
      "mean reward:27.794419970631424 loss: 14.788543701171875\n",
      "mean reward:27.726483357452967 loss: 14.77164363861084\n",
      "mean reward:27.79315263908702 loss: 14.764785766601562\n",
      "mean reward:27.869198312236286 loss: 14.723383903503418\n",
      "mean reward:27.825242718446603 loss: 14.796998977661133\n",
      "mean reward:27.78796169630643 loss: 14.747629165649414\n",
      "mean reward:27.828609986504723 loss: 14.770049095153809\n",
      "mean reward:27.802929427430094 loss: 14.794492721557617\n",
      "mean reward:27.781865965834427 loss: 14.748565673828125\n",
      "mean reward:27.71076523994812 loss: 14.759384155273438\n",
      "mean reward:27.759282970550576 loss: 14.731545448303223\n",
      "mean reward:27.776232616940582 loss: 14.732059478759766\n",
      "mean reward:27.800249687890137 loss: 14.757963180541992\n",
      "mean reward:27.870530209617755 loss: 14.773103713989258\n",
      "mean reward:27.99634591961023 loss: 14.743847846984863\n",
      "mean reward:28.015643802647414 loss: 14.734950065612793\n",
      "mean reward:27.99524375743163 loss: 14.764068603515625\n",
      "mean reward:27.956521739130434 loss: 14.73773193359375\n",
      "mean reward:27.96747967479675 loss: 14.794946670532227\n",
      "mean reward:27.9609644087256 loss: 14.777423858642578\n",
      "mean reward:27.99205448354143 loss: 14.773055076599121\n",
      "mean reward:27.95959595959596 loss: 14.79721450805664\n",
      "mean reward:28.036625971143174 loss: 14.78018856048584\n",
      "mean reward:28.037321624588365 loss: 14.75624942779541\n",
      "mean reward:28.0 loss: 14.810145378112793\n",
      "mean reward:28.020408163265305 loss: 14.794859886169434\n",
      "mean reward:28.005313496280554 loss: 14.746182441711426\n",
      "mean reward:27.933753943217667 loss: 14.738337516784668\n",
      "mean reward:27.895941727367326 loss: 14.780083656311035\n",
      "mean reward:27.85169927909372 loss: 14.765023231506348\n",
      "mean reward:27.884811416921508 loss: 14.761393547058105\n",
      "mean reward:27.899091826437942 loss: 14.76512622833252\n"
     ]
    }
   ],
   "source": [
    "cost_f = cost_f.cuda()\n",
    "policy.actor.train()\n",
    "cost_f. train()\n",
    "return_list, sum_of_cost_list, policy_loss = [], [], []\n",
    "D_sample_states = np.array([])\n",
    "D_sample_actions = np.array([])\n",
    "D_sample_probs = np.array([])\n",
    "for i in range(1000):\n",
    "    #trajs = [policy.generate_session(env) for _ in range(EPISODES_TO_PLAY)]\n",
    "    #sample_trajs = trajs + sample_trajs\n",
    "    #D_samp = preprocess_traj(trajs, D_samp)\n",
    "    state, prob, action, rewards = policy.generate_session(env)\n",
    "    D_sample_states = np.concatenate((D_sample_states, state.reshape(-1)))\n",
    "    D_sample_actions = np.concatenate((D_sample_actions, action))\n",
    "    D_sample_probs = np.concatenate((D_sample_probs, prob))\n",
    "    # UPDATING REWARD FUNCTION (TAKES IN D_samp, D_demo)\n",
    "    loss_rew = []\n",
    "    for _ in range(REWARD_FUNCTION_UPDATE):\n",
    "        D_samp = D_sample_states.reshape(-1,5)\n",
    "        selected_samp = np.random.choice(len(D_samp), DEMO_BATCH)\n",
    "        selected_demo = np.random.choice(len(D_demo_states), DEMO_BATCH)\n",
    "\n",
    "        D_s_samp_states  = D_samp[selected_samp]\n",
    "        D_s_samp_actions = D_sample_actions[selected_samp]\n",
    "        D_s_samp_probs   = D_sample_probs[selected_samp]\n",
    "        D_s_demo_states  = D_demo_states[selected_demo]\n",
    "        D_s_demo_actions = D_demo_actions[selected_demo]\n",
    "        D_s_demo_probs   = D_demo_probs[selected_demo]\n",
    "\n",
    "        #D̂ samp ← D̂ demo ∪ D̂ samp\n",
    "        states  = np.concatenate((D_s_demo_states, D_s_samp_states), axis = 0)\n",
    "        actions = np.concatenate((D_s_demo_actions, D_s_samp_actions), axis = 0)\n",
    "        probs   = np.concatenate((D_s_demo_probs, D_s_samp_probs), axis = 0)\n",
    "\n",
    "#         states, probs, actions = D_s_samp[:,:-2], D_s_samp[:,-2], D_s_samp[:,-1]\n",
    "#         states_expert, actions_expert = D_s_demo[:,:-2], D_s_demo[:,-1]\n",
    "        states_expert = D_s_demo_states\n",
    "        actions_expert = D_s_demo_actions\n",
    "\n",
    "        # Reducing from float64 to float32 for making computaton faster\n",
    "        states = torch.tensor(states, dtype=torch.float32).cuda()\n",
    "        probs = torch.tensor(probs, dtype=torch.float32).cuda()\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).cuda()\n",
    "        states_expert = torch.tensor(states_expert, dtype=torch.float32).cuda()\n",
    "        actions_expert = torch.tensor(actions_expert, dtype=torch.float32).cuda()\n",
    "\n",
    "        costs_samp = cost_f(torch.cat((states, actions.reshape(-1, 1)), dim=-1))\n",
    "        costs_demo = cost_f(torch.cat((states_expert, actions_expert.reshape(-1, 1)), dim=-1))\n",
    "\n",
    "        probs = torch.exp(probs)\n",
    "        # LOSS CALCULATION FOR IOC (COST FUNCTION)\n",
    "        loss_IOC = torch.mean(costs_demo) + \\\n",
    "                torch.log(torch.mean(torch.exp(-costs_samp)/(probs+1e-7)))\n",
    "        # UPDATING THE COST FUNCTION\n",
    "        cost_optimizer.zero_grad()\n",
    "        loss_IOC.backward()\n",
    "        cost_optimizer.step()\n",
    "\n",
    "        loss_rew.append(loss_IOC.detach().cpu().numpy())\n",
    "\n",
    "    for _ in range(EPISODES_TO_PLAY):\n",
    "        \n",
    "        states = torch.tensor(state, dtype=torch.float32).cuda()\n",
    "        actions = torch.tensor(action, dtype=torch.float32).cuda()\n",
    "            \n",
    "        costs = cost_f(torch.cat((states, actions.reshape(-1, 1)), dim=-1)).detach().cpu().numpy()\n",
    "        cumulative_returns = np.array(get_cumulative_rewards(costs, 0.99))\n",
    "        cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32).cuda()\n",
    "\n",
    "        mu, sigma = policy.actor(states)\n",
    "        distribution = Normal(mu, sigma)\n",
    "#         logits = policy(states)\n",
    "#         probs = nn.functional.softmax(logits, -1)\n",
    "#         log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "#         log_probs_for_actions = torch.sum(\n",
    "#             log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "        log_probs -= torch.log(1-actions.pow(2)+policy.actor.reparam_noise)\n",
    "        log_probs = log_probs.sum(1, keepdim=True)\n",
    "        \n",
    "        probs = torch.exp(log_probs)\n",
    "    \n",
    "        entropy = torch.mean(probs*log_probs)\n",
    "        loss = -torch.mean(log_probs*cumulative_returns -entropy*1e-2) \n",
    "\n",
    "        # UPDATING THE POLICY NETWORK\n",
    "        policy_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        policy_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    returns = np.sum(rewards)\n",
    "    sum_of_cost = np.sum(costs)\n",
    "    return_list.append(returns)\n",
    "    sum_of_cost_list.append(sum_of_cost)\n",
    "\n",
    "    mean_rewards.append(np.mean(return_list))\n",
    "    mean_costs.append(np.mean(sum_of_cost_list))\n",
    "    mean_loss_rew.append(np.mean(loss_rew))\n",
    "    mean_policy_loss.append(np.mean(policy_loss))\n",
    "\n",
    "    # PLOTTING PERFORMANCE\n",
    "    if i % 10 == 0:\n",
    "        # clear_output(True)\n",
    "        print(f\"mean reward:{np.mean(return_list)} loss: {loss_IOC}\")\n",
    "\n",
    "        plt.figure(figsize=[16, 12])\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(f\"Mean reward per {EPISODES_TO_PLAY} games\")\n",
    "        plt.plot(mean_rewards)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(f\"Mean cost per {EPISODES_TO_PLAY} games\")\n",
    "        plt.plot(mean_costs)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(f\"Mean loss per {REWARD_FUNCTION_UPDATE} batches\")\n",
    "        plt.plot(mean_loss_rew)\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(f\"Mean policy loss per {EPISODES_TO_PLAY} games\")\n",
    "        plt.plot(mean_policy_loss)\n",
    "        plt.grid()\n",
    "\n",
    "        # plt.show()\n",
    "        plt.savefig('plots/GCL_learning_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    if np.mean(return_list) > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-latest",
   "language": "python",
   "name": "ml-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
